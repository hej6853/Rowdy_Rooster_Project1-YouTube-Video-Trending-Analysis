{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import itertools\n",
    "import nltk\n",
    "import chardet\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file\n",
    "df = pd.read_csv('us_gb_ca_merged.csv') \n",
    "\n",
    "#split df into 3 countireis\n",
    "df_us = df[df['country'] == 'US']\n",
    "df_gb = df[df['country'] == 'GB']\n",
    "df_ca = df[df['country'] == 'CA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top video categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>title content</th>\n",
       "      <th>tag content</th>\n",
       "      <th>description content</th>\n",
       "      <th>total count title</th>\n",
       "      <th>total count tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53108</th>\n",
       "      <td>0</td>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "      <td>i left youtube for a month and this is what ha...</td>\n",
       "      <td>2020-08-11T16:34:06Z</td>\n",
       "      <td>jacksepticeye</td>\n",
       "      <td>24</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>jacksepticeye|funny|funny meme|memes|jacksepti...</td>\n",
       "      <td>2038853</td>\n",
       "      <td>353790</td>\n",
       "      <td>2628</td>\n",
       "      <td>40228</td>\n",
       "      <td>i left youtube for a month and this is what ha...</td>\n",
       "      <td>GB</td>\n",
       "      <td>['i', 'left', 'youtube', 'for', 'a', 'month', ...</td>\n",
       "      <td>['jacksepticeye', 'funny', 'funny meme', 'meme...</td>\n",
       "      <td>['i', 'left', 'youtube', 'for', 'a', 'month', ...</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53109</th>\n",
       "      <td>1</td>\n",
       "      <td>9nidKH8cM38</td>\n",
       "      <td>taxi cab slayer kills 'to know how it feels'</td>\n",
       "      <td>2020-08-11T20:00:45Z</td>\n",
       "      <td>Eleanor Neale</td>\n",
       "      <td>27</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>eleanor|neale|eleanor neale|eleanor neale true...</td>\n",
       "      <td>236830</td>\n",
       "      <td>16423</td>\n",
       "      <td>209</td>\n",
       "      <td>1642</td>\n",
       "      <td>the first 1000 people to click the link will g...</td>\n",
       "      <td>GB</td>\n",
       "      <td>['taxi', 'cab', 'slayer', 'kills', \"'to\", 'kno...</td>\n",
       "      <td>['eleanor', 'neale', 'eleanor neale', 'eleanor...</td>\n",
       "      <td>['the', 'first', '1000', 'people', 'to', 'clic...</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53110</th>\n",
       "      <td>2</td>\n",
       "      <td>M9Pmf9AB4Mo</td>\n",
       "      <td>apex legends | stories from the outlands – “th...</td>\n",
       "      <td>2020-08-11T17:00:10Z</td>\n",
       "      <td>Apex Legends</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>apex legends|apex legends characters|new apex ...</td>\n",
       "      <td>2381688</td>\n",
       "      <td>146739</td>\n",
       "      <td>2794</td>\n",
       "      <td>16549</td>\n",
       "      <td>while running her own modding shop, ramya pare...</td>\n",
       "      <td>GB</td>\n",
       "      <td>['apex', 'legends', '|', 'stories', 'from', 't...</td>\n",
       "      <td>['apex legends', 'apex legends characters', 'n...</td>\n",
       "      <td>['while', 'running', 'her', 'own', 'modding', ...</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53111</th>\n",
       "      <td>3</td>\n",
       "      <td>kgUV1MaD_M8</td>\n",
       "      <td>nines - clout (official video)</td>\n",
       "      <td>2020-08-10T18:30:28Z</td>\n",
       "      <td>Nines</td>\n",
       "      <td>24</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>nines|trapper of the year|crop circle|nines tr...</td>\n",
       "      <td>613785</td>\n",
       "      <td>37567</td>\n",
       "      <td>669</td>\n",
       "      <td>2101</td>\n",
       "      <td>nines - clout (official video)listen to clout ...</td>\n",
       "      <td>GB</td>\n",
       "      <td>['nines', '-', 'clout', '(official', 'video)']</td>\n",
       "      <td>['nines', 'trapper of the year', 'crop circle'...</td>\n",
       "      <td>['nines', '-', 'clout', '(official', 'video)li...</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53112</th>\n",
       "      <td>4</td>\n",
       "      <td>49Z6Mv4_WCA</td>\n",
       "      <td>i don't know what im doing anymore</td>\n",
       "      <td>2020-08-11T20:24:34Z</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>940036</td>\n",
       "      <td>87113</td>\n",
       "      <td>1860</td>\n",
       "      <td>7052</td>\n",
       "      <td>ssend love to my sponsor; for a super limited ...</td>\n",
       "      <td>GB</td>\n",
       "      <td>['i', \"don't\", 'know', 'what', 'im', 'doing', ...</td>\n",
       "      <td>['[none]']</td>\n",
       "      <td>['ssend', 'love', 'to', 'my', 'sponsor;', 'for...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     video_id  \\\n",
       "53108           0  J78aPJ3VyNs   \n",
       "53109           1  9nidKH8cM38   \n",
       "53110           2  M9Pmf9AB4Mo   \n",
       "53111           3  kgUV1MaD_M8   \n",
       "53112           4  49Z6Mv4_WCA   \n",
       "\n",
       "                                                   title  \\\n",
       "53108  i left youtube for a month and this is what ha...   \n",
       "53109       taxi cab slayer kills 'to know how it feels'   \n",
       "53110  apex legends | stories from the outlands – “th...   \n",
       "53111                     nines - clout (official video)   \n",
       "53112                 i don't know what im doing anymore   \n",
       "\n",
       "                publishedAt   channelTitle  categoryId         trending_date  \\\n",
       "53108  2020-08-11T16:34:06Z  jacksepticeye          24  2020-08-12T00:00:00Z   \n",
       "53109  2020-08-11T20:00:45Z  Eleanor Neale          27  2020-08-12T00:00:00Z   \n",
       "53110  2020-08-11T17:00:10Z   Apex Legends          20  2020-08-12T00:00:00Z   \n",
       "53111  2020-08-10T18:30:28Z          Nines          24  2020-08-12T00:00:00Z   \n",
       "53112  2020-08-11T20:24:34Z   CaseyNeistat          22  2020-08-12T00:00:00Z   \n",
       "\n",
       "                                                    tags  view_count   likes  \\\n",
       "53108  jacksepticeye|funny|funny meme|memes|jacksepti...     2038853  353790   \n",
       "53109  eleanor|neale|eleanor neale|eleanor neale true...      236830   16423   \n",
       "53110  apex legends|apex legends characters|new apex ...     2381688  146739   \n",
       "53111  nines|trapper of the year|crop circle|nines tr...      613785   37567   \n",
       "53112                                             [none]      940036   87113   \n",
       "\n",
       "       dislikes  comment_count  \\\n",
       "53108      2628          40228   \n",
       "53109       209           1642   \n",
       "53110      2794          16549   \n",
       "53111       669           2101   \n",
       "53112      1860           7052   \n",
       "\n",
       "                                             description country  \\\n",
       "53108  i left youtube for a month and this is what ha...      GB   \n",
       "53109  the first 1000 people to click the link will g...      GB   \n",
       "53110  while running her own modding shop, ramya pare...      GB   \n",
       "53111  nines - clout (official video)listen to clout ...      GB   \n",
       "53112  ssend love to my sponsor; for a super limited ...      GB   \n",
       "\n",
       "                                           title content  \\\n",
       "53108  ['i', 'left', 'youtube', 'for', 'a', 'month', ...   \n",
       "53109  ['taxi', 'cab', 'slayer', 'kills', \"'to\", 'kno...   \n",
       "53110  ['apex', 'legends', '|', 'stories', 'from', 't...   \n",
       "53111     ['nines', '-', 'clout', '(official', 'video)']   \n",
       "53112  ['i', \"don't\", 'know', 'what', 'im', 'doing', ...   \n",
       "\n",
       "                                             tag content  \\\n",
       "53108  ['jacksepticeye', 'funny', 'funny meme', 'meme...   \n",
       "53109  ['eleanor', 'neale', 'eleanor neale', 'eleanor...   \n",
       "53110  ['apex legends', 'apex legends characters', 'n...   \n",
       "53111  ['nines', 'trapper of the year', 'crop circle'...   \n",
       "53112                                         ['[none]']   \n",
       "\n",
       "                                     description content  total count title  \\\n",
       "53108  ['i', 'left', 'youtube', 'for', 'a', 'month', ...                 11   \n",
       "53109  ['the', 'first', '1000', 'people', 'to', 'clic...                  9   \n",
       "53110  ['while', 'running', 'her', 'own', 'modding', ...                 10   \n",
       "53111  ['nines', '-', 'clout', '(official', 'video)li...                  5   \n",
       "53112  ['ssend', 'love', 'to', 'my', 'sponsor;', 'for...                  7   \n",
       "\n",
       "       total count tag  \n",
       "53108               30  \n",
       "53109               43  \n",
       "53110               25  \n",
       "53111               19  \n",
       "53112                1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df[df['title'] == 'coney']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gb category video count vs top views : 10: music, 24:Entertainment, 20: gaming\n",
    "df_gb_topcategory = pd.DataFrame(df_gb.groupby('categoryId')['view_count'].sum()).sort_values(by = 'view_count',ascending=False).reset_index()\n",
    "video_count = pd.DataFrame(df_gb['categoryId'].value_counts()).reset_index().rename(columns={'index' : 'categoryId',\n",
    "                                                                                             'categoryId':'video_count'})\n",
    "df_gb_topcategory = df_gb_topcategory.merge(video_count, how = 'inner', on = 'categoryId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split categories\n",
    "df_gb_10 = df_gb[df_gb['categoryId'] == 10]\n",
    "df_gb_24 = df_gb[df_gb['categoryId'] == 24]\n",
    "df_gb_20 =  df_gb[df_gb['categoryId'] == 20]\n",
    "df_gb_25 =  df_gb[df_gb['categoryId'] == 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extracting hot topics with NLTK\n",
    "text = df_gb_25['tags'].str.lower().replace('|', ' ').str.cat(sep=' ')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "        \n",
    "# Stemming with NLTK\n",
    "Stem_words = []\n",
    "ps =PorterStemmer()\n",
    "for w in filtered_sentence:\n",
    "    rootWord=ps.stem(w)\n",
    "    Stem_words.append(rootWord)\n",
    "    \n",
    "# Lemmatization with NLTK\n",
    "filtered_sentence = list(filter(lambda token: token not in string.punctuation, filtered_sentence))\n",
    "filtered_sentence\n",
    "\n",
    "# remove unnecessay words\n",
    "stopwords = [\"'s\", \"’\", \"...\" , \"ft.\" , \"2\" ,\"x\" , \"1\", \"n't\", \"–\", \"3\", \"5\", \"4\",\n",
    "             \"2021\",\"2020\",\"trailer\", \"de\", \"official\", \"season\", \"video\", \"official\", \"season\", \"episode\",\"la\", \"le\", \"je\",\n",
    "             \"part\", \"je\", \"des\",\"world\",\"day\", \"10\",\"e\", \"avec\", \"‘\", \"à\", \"music\", \"none\", \"new\",\"lil\", \"like\", \"songs\", \"song\",\n",
    "            \"thee\",\"love\",\"bad\",\"g\",\"tv\", \"voice\",\"game\", \"news\",\"live\",\"watch\", \"full\", \"today\", \"uk\" ]\n",
    "for word in list(filtered_sentence):  # iterating on a copy since removing will mess things up\n",
    "    if word in stopwords:\n",
    "        filtered_sentence.remove(word)\n",
    "        \n",
    "#wordcloud\n",
    "word_could_dict=Counter(filtered_sentence)\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, background_color ='black',  stopwords = stopwords,\n",
    "                min_font_size = 10).generate_from_frequencies(word_could_dict)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "plt.savefig('gb25_tags_wordcloud.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>philip</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>johnson</th>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>election</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>british</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc|bbc</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harry</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|coronavirus</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prince</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus|coronavirus</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edinburgh</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eugenie</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online|watch</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biden|donald</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|news</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floyd</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funeral</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|us</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>royal</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tv|british</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|world</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shows|watch</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|finance</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meghan</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boseman</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|covid</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markle</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidential</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump|joe</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market|news</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station|breaking</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news|cable|cable</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latest</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philip|prince</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>licence</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lockdown|coronavirus</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lockdown|covid</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funeral|prince</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbs</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middleton</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "0                           \n",
       "philip                   233\n",
       "biden                    153\n",
       "johnson                  141\n",
       "election                 126\n",
       "british                  106\n",
       "bbc|bbc                  102\n",
       "harry                     86\n",
       "news|coronavirus          85\n",
       "prince                    81\n",
       "coronavirus|coronavirus   81\n",
       "covid                     74\n",
       "edinburgh                 73\n",
       "eugenie                   70\n",
       "online|watch              69\n",
       "biden|donald              67\n",
       "news|news                 65\n",
       "floyd                     63\n",
       "funeral                   59\n",
       "news|us                   59\n",
       "royal                     59\n",
       "tv|british                57\n",
       "united                    56\n",
       "president                 54\n",
       "family                    54\n",
       "news|world                53\n",
       "shows|watch               53\n",
       "trump                     51\n",
       "states                    51\n",
       "news|finance              50\n",
       "forecast                  50\n",
       "bbc                       49\n",
       "meghan                    46\n",
       "boseman                   45\n",
       "news|covid                45\n",
       "markle                    43\n",
       "presidential              42\n",
       "trump|joe                 42\n",
       "market|news               41\n",
       "station|breaking          41\n",
       "news|cable|cable          41\n",
       "latest                    39\n",
       "philip|prince             39\n",
       "licence                   38\n",
       "lockdown|coronavirus      38\n",
       "lockdown|covid            38\n",
       "show                      38\n",
       "funeral|prince            37\n",
       "cbs                       36\n",
       "middleton                 36\n",
       "coronavirus               36"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = pd.DataFrame(filtered_sentence)\n",
    "filtered_sentence_unique = pd.DataFrame(filtered_sentence.value_counts())\n",
    "filtered_sentence_unique = filtered_sentence_unique.rename(columns={'0':'count'})\n",
    "filtered_sentence_unique.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
